{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set defaults\n",
    "plt.style.use('seaborn-white')   # seaborn custom plot style\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))   # set default size/resolution\n",
    "plt.rc('font', size=12)   # font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 1\n",
    "\n",
    "# Sklearn transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The modeling pipeline\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/image_0.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The steps of the modeling pipeline\n",
    "\n",
    "1. Create features to best reflect the meaning behind data\n",
    "2. Create model appropriate to capture relationships between features\n",
    "    - e.g. linear, non-linear\n",
    "3. Select a loss function and fit the model (determine $\\hat{\\theta}$).\n",
    "4. Evaluate model (e.g. using RMSE)\n",
    "\n",
    "After these steps, use the model for prediction and/or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Software development and the modeling pipeline \n",
    "\n",
    "* Each step may contain complicated transformations and logic\n",
    "* The pipeline above represents a single attempt at a model\n",
    "    - May have thousands of feature/model/paramater combinations to choose from!\n",
    "    - Remember the Data Science Life Cycle!\n",
    "* ML pipelines: [the high interest credit card of technical debt](https://ai.google/research/pubs/pub43146)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features and Models using `Scikit Learn`\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "    \n",
    "* Scikit-Learn implements many common steps in the feature/model creation pipeline.\n",
    "* Interfaces with `numpy` arrays and Pandas dataframes (somewhat)\n",
    "    - Some work required keeping track of columns in scikit\n",
    "    \n",
    "    \n",
    "<img src=\"imgs/sklearn.png\" width=\"50%\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scikit-Learn feature transformers\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "\n",
    "<img src=\"imgs/feature_part.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"imgs/image_1.png\" width=\"50%\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scikit-Learn (linear) models\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "\n",
    "<img src=\"imgs/model_part.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "<img src=\"imgs/image_2.png\" width=\"50%\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note...\n",
    "\n",
    "- `sklearn` documentation is (usually) very, *very* good\n",
    "- there's a lot to learn, but lots of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-Learn Transformer Classes\n",
    "\n",
    "* Transformers process data and output features (transformed data).\n",
    "    - Input data should be a (multi-column) Numpy Array (`sklearn` coerces a dataframe using `.values`).\n",
    "    - Output data is also a Numpy Array.\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize with parameters| `binar = Binarizer(thresh)` | 'set x=0 if x < thresh, else 1'|\n",
    "|Transform data in a dataset | `feat = binar.transform(data)` | Binarize all columns in `data`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the transformer and use it in the dataset\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = Binarizer(threshold = 20)                     # initialize with the parameter\n",
    "binarized = bi.transform(tips[['total_bill']])     # called transform on a data \n",
    "binarized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if works\n",
    "# \n",
    "(\n",
    "    pd.concat([tips.total_bill, pd.DataFrame(binarized, columns=['binarized'])], axis=1)\n",
    "    .sort_values('total_bill')\n",
    "    .plot(x='total_bill', y='binarized')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some transformer classes require fitting\n",
    "\n",
    "* Transformation logic often requires some knowledge of the dataset before transforming.\n",
    "    - z-score: z = (x-μ)/σ, where x is the raw score, μ is the population mean, and σ is the population standard deviation.\n",
    "* These transformers must be *fit* to the data before use.\n",
    "* Typical usage: fit transformer on a sample; use that fit transformer to transform future data.\n",
    "\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize with parameters| `stdscaler = StandardScaler()` | z-scale the data (no parameters) |\n",
    "|Fit the transformer| `stdscaler.fit(data)` | compute the mean and std-dev of `data`|\n",
    "|Transform data in a dataset | `feat = stdscaler.transform(newdata)` | z-scale `newdata` with mean/stdev of `data`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "quantcols = ['total_bill', 'size', 'tip']\n",
    "tips[quantcols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work!\n",
    "stdscaler.transform(tips[quantcols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stdscaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.fit(tips[quantcols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scaled data\n",
    "stdscaler.transform(tips[quantcols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: One-Hot Encoding\n",
    "\n",
    "- `sklearn` provides a preprocessor to compute One-Hot encoding for categoricals\n",
    "- last time, we code to do this by hand : ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just categoricals\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tips_cat = ['sex', 'smoker', 'day', 'time']\n",
    "regdata = tips[tips_cat]\n",
    "regdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()  # create\n",
    "ohe.fit(regdata)       # fit to data\n",
    "ohe.categories_        # you can look into created categories ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ohe.transform(regdata)  # why toarray()? to avoid sparse matrix!\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.inverse_transform(features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 2\n",
    "\n",
    "# Sklearn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-Learn Model Classes\n",
    "\n",
    "`Sklearn` model classes (estimators) behave like transformers, but use outcomes (target variables, dependent variables that you train your model on) to fit and evaluate.\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize model parameters| `lr = LinearRegression()` | Create (empty) linear regression model|\n",
    "|Fit the model to the data | `lr.fit(data, outcomes)` | Determines regression coefficients|\n",
    "|Use model for prediction |`lr.predict(newdata)`| Use regression line make predictions|\n",
    "|Evaluate the model| `lr.score(data, outcomes)` | Calculate the $R^2$ of the LR model|\n",
    "|Access model attributes| `lr.coef_` | Access the regression coefficients|\n",
    "\n",
    "*Note:* Once `fit`, estimators are just transformers (`predict` <-> `transform`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(tips[['total_bill', 'size']], tips.tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can predict with it\n",
    "lr.predict(tips[['total_bill', 'size']])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression coefficients, why 2 slopes?\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "np.sqrt(np.mean((preds - tips.tip.values)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2\n",
    "lr.score(tips[['total_bill', 'size']], tips.tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building models with transformers and estimators\n",
    "\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "    \n",
    "1. Define your transformations; models.\n",
    "1. Transform input data to features.\n",
    "1. Use (transformed) features to fit model.\n",
    "1. Predict outcomes from features using fit model.\n",
    "\n",
    "<img src=\"imgs/image_0.png\" width=\"50%\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.toarray()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multicollinearity\n",
    "# which variables should be dropped?\n",
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(features, tips.tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher dim. plane of best fit in 10 dim  (slopes)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE: terrible model! (why?)\n",
    "np.sqrt(np.mean((preds - tips.tip.values)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add predictions to original data\n",
    "tips.assign(preds=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redundant Features\n",
    "\n",
    "- For any categorical feature, can always leave out one of the categories (inferred from the rest).\n",
    "- E.g., \"Yes\" and \"No\". Just have binary feature: \"Yes\".\n",
    "- Done with `drop='first'` in `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_better = OneHotEncoder(drop='first')\n",
    "features = ohe_better.fit_transform(tips[tips_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_better.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(features, tips.tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't affect RMSE, but makes the individual weights on features more stable\n",
    "np.sqrt(np.mean((preds - tips.tip.values)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 3\n",
    "\n",
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The \"entire model\"\n",
    "\n",
    "- The \"entire\" model is the combination of all preprocessing + estimation done to the raw data.\n",
    "- `sklearn` allows you to combine your preprocessing + estimation into a single \"pipeline\" object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting it together: Scikit-Learn Pipelines\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "\n",
    "* Put together transformers and models using `sklearn.Pipeline`.\n",
    "* Create a pipeline: `pl = Pipeline([feat_trans, mdl])`\n",
    "* Fit *all* the transformer(s)/model(s) in the pipeline using `pl.fit(data, target)`\n",
    "* Predict from *raw* input data through the pipeline using `pl.predict`.\n",
    "* Note: a fit pipeline is also a transformer!\n",
    "\n",
    "<img src=\"imgs/image_0.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass a list of feature trans. and models, in sequence.\n",
    "# does all fitting and transforming\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines are lists of steps: each is a transformation/estimator\n",
    "# each transformation is a tuple: the 'name' for the step name, and the transformer/estimator object.\n",
    "pl = Pipeline([\n",
    "    ('one-hot', OneHotEncoder()),\n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(regdata, tips.tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the 'steps' of the pipeline using .named_steps\n",
    "# gives a dictionary\n",
    "# key: name you gave\n",
    "# values: fit pipleline objects\n",
    "pl.named_steps['one-hot'].transform(regdata).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['one-hot'].categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(regdata)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 -- still terrible! (1 is good, 0 is bad)\n",
    "pl.score(regdata, tips.tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Realistic) Sklearn Pipelines\n",
    "<div class=\"image-txt-container\">\n",
    "    \n",
    "* `ColumnTransformer` was a recent addition (2018).\n",
    "* Transforms using multiple transformers, each on different columns.\n",
    "* `ColumnTransformer` performs the transformations and concatenates the output (axis=1).\n",
    "\n",
    "<img src=\"imgs/image_3.png\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split data up into quant. and cat. features\n",
    "# 2. z-scale for quant features\n",
    "# 3. One-hot encode for cat. features\n",
    "# 4. Two pipelines\n",
    "# 5. Use column transformer to put everything back\n",
    "# 6. Apply the model => predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.drop(['tip', 'total_bill', 'size'], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns and associated transformers\n",
    "num_feat = ['total_bill', 'size']\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('scaler', pp.StandardScaler())   # z-scale\n",
    "])\n",
    "\n",
    "# Categorical columns and associated transformers\n",
    "cat_feat = ['sex', 'smoker', 'day', 'time']\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('onehot', pp.OneHotEncoder())     # output from Ordinal becomes input to OneHot\n",
    "])\n",
    "\n",
    "# preprocessing pipeline (put them together)\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_feat),\n",
    "        ('cat', cat_transformer, cat_feat)\n",
    "    ])\n",
    "\n",
    "pl = Pipeline(steps=[('preprocessor', preproc), ('regressor', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(tips.drop('tip', axis=1), tips.tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pl.predict(tips.drop('tip', axis=1))\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((preds - tips.tip)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.score(tips.drop('tip', axis=1), tips.tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.named_steps['preprocessor'].transform(tips.drop('tip', axis=1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 4\n",
    "\n",
    "# Evaluating the fit model\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/image_4.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the quality of a model\n",
    "\n",
    "* Given a fit regressor on dataset, calculate e.g. the root-mean-square error.\n",
    "* If the error is low, do you think it's a good model?\n",
    "    - It fits the given *data* well, but is it a good model? (Is the sample representative?)\n",
    "    - Will it give good predictions on similar, unknown, data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamental Concepts of the quality of a 'fit model'\n",
    "\n",
    "* **Bias**: the expected deviation between the predicted value and true value\n",
    "* **Variance**: \n",
    "    - **Observation Variance**: the variability of the random noise in the process we are trying to model. \n",
    "    - **Estimated Model Variance**: the variability in the predicted value across different datasets. (Does the model generalize?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Quality: Bias and Variance\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "    \n",
    "* The red bulls-eye: the true behavior of DGP\n",
    "* Each dart: a specific function that models/predicts the DGP\n",
    "* The model parameters $\\theta$ select these functions.\n",
    "* Credit: Scott Fortmann-Roe\n",
    "    \n",
    "<img src=\"imgs/image_5.png\" width=\"100%\">\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the quality of a linear model\n",
    "\n",
    "Given a dataset on which to fit the regression coefficients:\n",
    "1. Calculate the RMSE to test for bias.\n",
    "2. To test for variance, bootstrap estimate the regression coefficients:\n",
    "    - sample the data.\n",
    "    - For each sample, calculate the linear predictor.\n",
    "    - For each input feature, calculate the CI for the distribution of predictions.\n",
    "    - Large \"prediction intervals\" imply the model is susceptible to noise (e.g. outliers)\n",
    "    \n",
    "Still, this relies on a \"representative sample\" for generalization to new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=tips, x='total_bill', y='tip');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the quality of a (general) model\n",
    "\n",
    "* Given a fit (non-linear) model, there are three possibilities for quality:\n",
    "    - The model doesn't fit the given data well (high bias; underfit)\n",
    "    - Does it reflect the process of interest? (good fit; robust)\n",
    "    - Does it just fit the data (noise and all)? (high variance; overfit)\n",
    "\n",
    "* How can we ascertain the quality on similar, out-of-sample data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the quality of a (general) model\n",
    "\n",
    "* Given a quadratic process, a linear model has high bias.\n",
    "* \"Connecting-the-dots\" will fail to generalize (high variance).\n",
    "* Balance model complexity with complexity of DGP.\n",
    "\n",
    "![overfit](imgs/under-over-fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: predicting survival on the Titanic with Decision Trees\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "\n",
    "* Did a given passenger survive the Titanic distaster?\n",
    "* The (simple) tree below has mediocre accuracy\n",
    "\n",
    "<img src=\"imgs/image_6.png\" width=\"50%\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reducing Bias with more complicated models\n",
    "\n",
    "* Improve performance by \"growing\" the decision tree model.\n",
    "* Decrease the number of passengers required in leaf nodes.\n",
    "* Effect: \"Learn\" individual passengers?\n",
    "* How do the know your model generalizes?\n",
    "\n",
    "<img src=\"imgs/Titanic_Decision_Tree.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train-Test Split\n",
    "\n",
    "To assess your model for overfitting to the data, randomly split the data into a \"training set\" and a \"test set\".\n",
    "\n",
    "<div class=\"image-txt-container\">\n",
    "\n",
    "* The training set is used to fit the model (train the predictor).\n",
    "* The test set is used to test the goodness-of-fit of the fit model.\n",
    "* *similar* to bootstrap estimating a regression model.\n",
    "\n",
    "<img src=\"imgs/train-test.png\">\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The machine learning training pipeline:\n",
    "\n",
    "<img src=\"imgs/train-test.png\" width=\"50%\">\n",
    "\n",
    "Scikit-Learn has functions that help us do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Scikit-Learn for train-test split\n",
    "\n",
    "* Splitting a dataset using `sklearn.model_selection.train_test_split` \n",
    "* Given features `X` and a target array `y`,\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "```\n",
    "randomly splits the features and target into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tips.drop('tip', axis=1)\n",
    "y = tips.tip\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(X_train)/len(X),\n",
    "    len(X_test)/len(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Prediction Pipeline\n",
    "\n",
    "* Train a simple linear regression model on the tips data\n",
    "* Split the data into a training and test set:\n",
    "    - fit the model on the training set\n",
    "    - compute the error on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips.drop(['tip', 'sex', 'smoker', 'day', 'time' ], axis=1)\n",
    "y = tips.tip\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "\n",
    "pl = Pipeline([\n",
    "   ('lin-reg', LinearRegression())\n",
    "])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# performance on training data\n",
    "pred_train = pl.predict(X_train)\n",
    "rmse_train = np.sqrt(np.mean((pred_train - y_train)**2))\n",
    "\n",
    "# performance on test data -- what we really care about\n",
    "pred_test = pl.predict(X_test)\n",
    "rmse_test = np.sqrt(np.mean((pred_test - y_test)**2))\n",
    "\n",
    "print (\"train RMSE: %s\" % rmse_train)\n",
    "print (\"test RMSE: %s\" % rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion: evaluating model fit\n",
    "\n",
    "* Complex models are required to model complex phenomena.\n",
    "* How can you tell a complex model isn't over-fitting to the data?\n",
    "    - Answer: split into a training set and a test set.\n",
    "- If test performance is << train performance, you've overfit"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
