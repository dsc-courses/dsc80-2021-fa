{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80 - Lab 02\n",
    "\n",
    "### Due Date: Monday October 11th, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook may be graded (if it contains free response questions or asks you to draw plots).\n",
    "\n",
    "**Note**: Labs will have public tests and private tests. The public \"smoke tests\" that you will run below and which appear on Gradescope are generally worth no points. After the due date, we will replace these tests with private tests that will determine your grade. This is different from DSC 10, where labs only had public tests!\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name.\n",
    "- If you changed something you weren't supposed to, just use git to revert! Ask us if you need help with this, or google around for `git revert`.\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for *projects* will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file. You can write code here, but make sure that all of your real work is in the .py file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional helper functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Test scores**\n",
    "\n",
    "You will be given a small dataset (so that you can manually check the correctness of your code). Please follow a few requirements when solving the problems below:\n",
    "\n",
    "* For all questions you need to write code general enough to be applied to another similar dataset. \n",
    "* Do not hard-code any answers please. \n",
    "* Do not use `for` or `while` loops\n",
    "   \n",
    "\n",
    "1. Write a function called `data_load` that takes a file name of the data set to be read as a string and returns a dataframe following the steps below:\n",
    "\n",
    "    a. Read only a subset of columns: `name`, `tries`, `highest_score`, `sex`\n",
    "    \n",
    "    b. Then you realized that for your analysis the column `sex` is not needed. Remove it. \n",
    "    \n",
    "    c. You want to customize the column names: rename `name` to `firstname` and `tries` to `attempts`\n",
    "    \n",
    "    d. Turn the `firstname` column into the index.\n",
    "\n",
    "\n",
    "2. Write a function `pass_fail` that takes the dataframe returned from the function above and adds a column `pass` based on the following conditions:\n",
    "\n",
    "    * \"Yes\" if a number of attempts is strictly less than 3 and the score is >= 50\n",
    "    * \"Yes\" if a number of attempts is strictly less than 6 and the score is >= 70\n",
    "    * \"Yes\" if a number of attempts is strictly less than 10 and the score is >= 90\n",
    "    * \"No\" otherwise\n",
    " \n",
    "Your function should return the (modified) input dataframe with the added column.\n",
    "    \n",
    "3. Write a fuction `av_score` that takes in a dataframe from the question above and returns the average score for those students who passed the test. \n",
    "    \n",
    "4. Write a function `highest_score_name` that takes in the dataframe from question 1.2 and returns a dictionary, where the key is the highest score and the value is the name (as a list) of the person with the highest score (attempts do not count). If more than one student got the highest score, include all names in a list. \n",
    "\n",
    "5. Write a function `idx_dup` that does not take any parameters and returns a single integer, answering the question below:\n",
    "\n",
    "Is it possible for a dataframe's index to have duplicate values?\n",
    "1. No, the index values must be unique and uses non-negative integers only, just like in numpy arrays\n",
    "2. No, the index values must be unique and uses integers only\n",
    "3. No, the index values must be unique but index values are not restricted to integers\n",
    "4. Yes, but index values must be non-negative integers only\n",
    "5. Yes, but index values must be integers only\n",
    "6. Yes and index values are not restricted to integers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "scores_fp = os.path.join('data', 'scores.csv')\n",
    "scores = data_load(scores_fp)\n",
    "passfail = pass_fail(scores.copy())\n",
    "avscore = av_score(passfail.copy())\n",
    "highest = highest_score_name(passfail)\n",
    "idxdup = idx_dup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricky Pandas.\n",
    "\n",
    "Sometimes you can get input that you do not expect. The next set of questions walk you through a few examples that might surprise you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 : Duplicate and selection**\n",
    "\n",
    "\n",
    "\n",
    "1. Write a function `trick_me` that does not take any parameters. <br>Inside the function: \n",
    "    * Create a dataframe `tricky_1` that has three columns labeled: \"Name\", \"Name\", \"Age\". Your table should have 5 rows, the values are up to you. \n",
    "    * Save this dataframe in the `csv` file called `tricky_1.csv` without the index. \n",
    "    * Now create another dataframe, `tricky_2`, by reading in the file `tricky_1.csv `. What are your observations?\n",
    "        1. It was not possible to create a dataframe with the duplicate columns\n",
    "        2. `tricky_1` and `tricky_2` have the same column names\n",
    "        3. `tricky_1` and `tricky_2` have different column names\n",
    "    * Return your answer as a letter\n",
    "    \n",
    "    \n",
    "\n",
    "2. Write a function `reason_dup` that answers the following question: `Why does pandas allow us to have duplicate column names?` by returning a corresponding letter. \n",
    "    1. It does not, duplicate column names are not allowed\n",
    "    2. Since duplicate indices are allowed and we also can transpose a dataframe.\n",
    "    3. It is a bug in Pandas\n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "3. Write a function `trick_bool` that does not take any parameters. To determine the correct answers from the list below, you should follow the steps outlined by experimenting in *the notebook* (or a python REPL). Outside the function:\n",
    "    * Create a dataframe `bools` that has four columns labeled: \"True\", \"True\", \"False\", \"False\". Each column name is boolean.\n",
    "    * Your table should have 4 rows, the values are up to you. \n",
    "    * You need to think (without running it) what output you should get when running each line of code below. Pick a corresponding answer from a given list. Your function should return a list with three letters that correspond to the dataframe structure for each line below. \n",
    "    \n",
    "     ```\n",
    "     df[True]\n",
    "     df[[True, True, False, False]]\n",
    "     df[[True, False]]\n",
    "     ```\n",
    "    \n",
    "        1. Dataframe: 2 columns, 1 row\n",
    "        2. Dataframe: 2 columns, 2 rows\n",
    "        3. Dataframe: 2 columns, 3 rows\n",
    "        4. Dataframe: 2 columns, 4 rows\n",
    "        5. Dataframe: 3 columns, 1 rows\n",
    "        6. Dataframe: 3 columns, 2 rows\n",
    "        7. Dataframe: 3 columns, 3 rows\n",
    "        8. Dataframe: 3 columns, 4 rows\n",
    "        9. Dataframe: 4 columns, 1 rows\n",
    "        10. Dataframe: 4 columns, 2 rows\n",
    "        11. Dataframe: 4 columns, 3 rows\n",
    "        12. Dataframe: 4 columns, 4 rows\n",
    "        13. Error\n",
    "    \n",
    "    \n",
    "4.  Write a function `reason_bool` that answers the following question: `Why the outputs are the way they are?` by returning a corresponding letter. \n",
    "    1. booleans arrays select either rows or columns, randomly\n",
    "    2. booleans arrays always select rows by default\n",
    "    3. booleans arrays always select columns by default \n",
    "    4. booleans arrays always select rows by default, unless column names are set to `True`/`False` values.\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "trick_ans = trick_bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 : np.NaN in a dataframe**\n",
    "\n",
    "\n",
    "In the notebook, use the code given below to create a dataframe called `nans`. Note that we use `np.NaN` (`numpy`'s representation of 'Not a Number') to create missing values.\n",
    " \n",
    "```\n",
    "nans = pd.DataFrame([[0,1,np.NaN], [np.NaN, np.NaN, np.NaN], [1, 2, 3]])\n",
    "```\n",
    "Now you decided to make your dataset more readable for people who do not understand `NaN` and replace it with a `MISSING` string instead. In order to do that you wrote the following function:\n",
    "\n",
    "```\n",
    "def change(x):\n",
    "    if x == np.NaN:\n",
    "        return \"MISSING\"\n",
    "    else:\n",
    "        return x\n",
    "```\n",
    "\n",
    "* Write a line of code that applies the function above to the last column of the `nans` dataframe. \n",
    "* What was a result?\n",
    "    * A: It worked: all np.NaNs in the last columns where changed to \"MISSING\"\n",
    "    * B: It did not work: does not matter how I tried, the NaN values were not changed.\n",
    "    \n",
    "I expect you to answer `B` here. What had happened? Turns out, you can't use simple comparison `==` to detect if a value is `np.NaN`. You need to use another way to compare a variable to a `np.NaN`, read about it [here](https://stackoverflow.com/questions/41342609/the-difference-between-comparison-to-np-nan-and-isnull)\n",
    "\n",
    "1. Modify the function `change` above to work as expected.\n",
    "2. Write method `correct_replacement` that takes in a dataframe like `nans` and returns a modified dataframe, where all the `NaN` are replaced with `\"MISSING\"`. Use your corrected version of `change` to do this. **The pandas function .fillna is not allowed in this question.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "In this question you will create two general purpose functions that make it easy to 'qualitatively' assess the contents of a dataframe.\n",
    "\n",
    "1. Create a function `population_stats` which takes in a dataframe `df` and returns a dataframe indexed by the columns of `df`, with the following columns:\n",
    "    * `num_nonnull` contains the number of non-null entries in each column,\n",
    "    * `pct_nonnull` contains the proportion of entries in each column that are non-null,\n",
    "    * `num_distinct` contains the number of distinct entries in each column,\n",
    "    * `pct_distinct` contains the proportion of (non-null) entries in each column that are distinct from each other.\n",
    "    \n",
    "*Note*: you may find the `.nunique()` series method useful.\n",
    "\n",
    "*Note*: The number of distinct entries does not include nulls.\n",
    "    \n",
    "2. Create a function `most_common` which takes in a dataframe `df` and a number `N` and returns a dataframe of the `N` most-common values (and their counts) for each column of `df`. Any column with fewer than `N` distinct values should contain `NaN` in those entries. \n",
    "\n",
    "*Note*: you can loop through the *columns* of `df` to construct your output. You should **not** be looping through rows.\n",
    "\n",
    "For example, for the subset of the `salaries` dataframe with columns 'Job Title' and 'Status' from lecture one (left), `most_common(salaries, N=5)` is given (right). \n",
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"data/imgs/dataframe.png\" width=\"70%\"/></td>\n",
    "    <td><img src=\"data/imgs/most_common.png\" width=\"70%\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "pop_data = np.random.choice(range(10), size=(100, 4))\n",
    "df_pop = pd.DataFrame(pop_data, columns='A B C D'.split())\n",
    "out_pop = population_stats(df_pop)\n",
    "\n",
    "common_data = np.random.choice(range(10), size=(100, 2))\n",
    "common_df = pd.DataFrame(common_data, columns='A B'.split())\n",
    "common_out = most_common(common_df, N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "## Faulty Scooters\n",
    "\n",
    "A new electric scooter company 'Maxwell Scooters' opened a retail shop in La Jolla recently and 300 UCSD students bought new scooters for getting around campus. After 8 students start complaining their scooters are faulty, negative on-line reviews for the scooters start to spread. In response, the scooter company adamantly claims that 99% of their scooters come off the production line working properly. You think this seems unlikely and decide to investigate.\n",
    "\n",
    "* Select a significance level for you investigation. (Not to be turned in)\n",
    "* What are reasonable choices for the *Null Hypothesis* for your investigation? Select all that apply:\n",
    "    1. The scooter company produces scooters that are 99% non-faulty.\n",
    "    2. The scooter company produces scooters that are less than 99% non-faulty.\n",
    "    3. The scooter company produces scooters that are at least 1% faulty.\n",
    "    4. The scooter company produces scooters that are ~2.6% faulty.\n",
    "\n",
    "Return your answer in a function `null_hypoth` that takes zero arguments.\n",
    "\n",
    "* Create a function `simulate_null` simulates a single step of data generation under the null hypothesis. The function should return a binary array.\n",
    "\n",
    "* Create a function `estimate_p_val` that takes in a number `N` and returns the estimated p-value of your investigation upon simulating the null hypothesis `N` times.\n",
    "\n",
    "*Note*: Plot the Null distribution and your observed statistic to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-Heroes\n",
    "\n",
    "The questions below analyze a dataset of super-heroes found in the `data` directory. One of the datasets have a list of attributes on each super-hero, while the other is a *boolean* dataframe of which super-heroes have which super-powers. Note, the datasets contain information on both *good* super-heroes, as well as *bad* super-heroes (AKA villains). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "### Super-hero powers\n",
    "\n",
    "Now read in the dataset of super-hero powers in the `data` directory. Create a function `super_hero_powers` that takes in a dataframe like `powers` and returns a list with the following three entries:\n",
    "\n",
    "1. The name of the super-hero with the greatest number of powers.\n",
    "2. The name of the most common super-power among super-heroes whose names begin with 'M'.\n",
    "3. The most popular super-power among those with only one super-power.\n",
    "\n",
    "You should *not* be hard-coding your answers in this question; your function should work on any dataset similar to `powers`. You should not be using loops in this question.\n",
    "\n",
    "*Note:* You may find the `.idxmax` method useful in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "super_fp = os.path.join('data', 'superheroes_powers.csv')\n",
    "powers = pd.read_csv(super_fp)\n",
    "super_out = super_hero_powers(powers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "### Super-hero attributes\n",
    "\n",
    "Read in the dataset of super-hero attributes from the `data` directory. Use your summary functions from question 4 to help acquaint yourself with the dataset.\n",
    "\n",
    "Cleaning the data: the dataset has no explicit null (`np.NaN`) values, although many entries *should* be null. Replace these values with null by creating a function `clean_heroes`.\n",
    "\n",
    "Now answer the following questions, collecting your answers in a (function `super_hero_stats` that returns) a list. You should answer the questions using the *cleaned* super-heroes data; your answers *should* be hard-coded in the function.\n",
    "1. Which publisher has a greater proportion of 'bad' characters -- Marvel Comics or DC Comics?\n",
    "2. Give the number of characters that are NOT human, or the publisher is not Marvel Comics nor DC comics. For this question, only consider race \"Human\" as human, races such as \"Human / Radiation\" don't count as human.\n",
    "3. Give the name of the character that's both greater than one standard deviation above mean in height and at least one standard deviation below the mean in weight.\n",
    "4. Who is heavier on average: good or bad characters?\n",
    "5. What is the name of the tallest Mutant with no hair?\n",
    "6. What is the probability that a randomly chosen 'Marvel' character in the dataset is a woman?\n",
    "\n",
    "*Note:* Since your answers to these questions should be hard-coded, you should not include your code in your .py file. Just return a list with your answers.\n",
    "\n",
    "*Note:* Nan denotes an unknown value that does not count as an entry with any attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "superheroes_fp = os.path.join('data', 'superheroes.csv')\n",
    "heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "clean_out = clean_heroes(heroes)\n",
    "\n",
    "stats_out = super_hero_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "### Are blond-haired, blue-eyed characters disproportionately 'good'?\n",
    "\n",
    "1. Create a function `bhbe` ('blond-hair-blue-eyes') that returns a boolean column that labels super-heroes/villains that are blond-haired *and* blue eyed.\n",
    "    * Look at the values of the hair/eyes columns; it needs some cleaning! (The doctest makes sure you've cleaned it properly. If you don't pass the doctest, look more closely at the values in the columns!)\n",
    "\n",
    "\n",
    "Now, you'd like to answer the question \n",
    "> \"Are blond-haired, blue-eyed characters disproportionately 'good'?\"\n",
    "\n",
    "To do this, you'd like to test the null hypothesis:\n",
    "> \"The proportion of 'good' heroes among blond-haired, blue-eyed heroes is roughly the same as (equals) the proportion of 'good' heroes in the overall population.\"\n",
    "\n",
    "Fix a significance level of 1%.\n",
    "\n",
    "2. Create a function `observed_stat` that takes in `heroes`, and returns the observed test statistic.\n",
    "3. Create a function `simulate_bhbe_null` that takes in a number `n` that returns a `n` instances of the test statistic generated under the null hypothesis. You should hard-code your simulation parameter into the function (rounding to the nearest hundredth is fine); the function should *not* read in any data.\n",
    "4. Create a function `calc_pval` that returns a list where:\n",
    "    * the first element is the p-value for hypothesis test (using 100,000 simulations). Please run the code yourself and hard-code this answer, as actually running the 100,000 simulation hypothesis test will timeout on gradescope. \n",
    "    * the second element is `Reject` if you reject the null hypothesis and `Fail to reject` if you fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "superheroes_fp = os.path.join('data', 'superheroes.csv')\n",
    "heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "bhbe_out = bhbe_col(heroes)\n",
    "\n",
    "obs_stat_out = observed_stat(heroes)\n",
    "\n",
    "simulate_bhbe_out = simulate_bhbe_null(10)\n",
    "\n",
    "pval_out = calc_pval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Line\n",
    "\n",
    "Before submitting your lab, make sure to run the doctests in the terminal with `python -m doctest lab.py`. If all of the tests in the notebook pass, but some fail when uploading to Gradescope, make sure that you've run the doctests in the terminal and they all pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(scores, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(scores.columns)\n['attempts', 'highest_score']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(scores.index[0], int)\nFalse",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(passfail, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(passfail.columns)\n3",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> passfail.loc[\"Julia\", \"pass\"] == 'Yes'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(avscore, float)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 91 < avscore < 92\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(highest, dict)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(next(iter(highest.items()))[1])\n3",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(idxdup, int)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 1 <= idxdup <= 6\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> ans =  trick_me()\n>>> ans == 'A' or ans == 'B' or ans == \"C\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ans = reason_dup()\n>>> ans == 'A' or ans == 'B' or ans == \"C\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(trick_ans, list)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(trick_ans[1], str)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ans = reason_bool()\n>>> ans == 'A' or ans == 'B' or ans == \"C\" or ans ==\"D\"\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> change(1.0) == 1.0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> change(np.NaN) == 'MISSING'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> nans = pd.DataFrame([[0,1,np.NaN], [np.NaN, np.NaN, np.NaN], [1, 2, 3]])\n>>> A = correct_replacement(nans)\n>>> (A.values == 'MISSING').sum() == 4\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out_pop.index.tolist() == ['A', 'B', 'C', 'D']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> cols = ['num_nonnull', 'pct_nonnull', 'num_distinct', 'pct_distinct']\n>>> out_pop.columns.tolist() == cols\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (out_pop['num_distinct'] <= 10).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (out_pop['pct_nonnull'] == 1.0).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> common_out.index.tolist() == [0, 1, 2]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> common_out.columns.tolist() == ['A_values', 'A_counts', 'B_values', 'B_counts']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> common_out['A_values'].isin(range(10)).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(null_hypoth(), list)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(null_hypoth()).issubset({1,2,3,4})\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> pd.Series(simulate_null()).isin([0,1]).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 < estimate_p_val(1000) < 0.1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(super_out, list)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(super_out)\n3",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all([isinstance(x, str) for x in super_out])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> clean_out['Skin color'].isnull().any()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> clean_out['Weight'].isnull().any()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> stats_out[0] in ['Marvel Comics', 'DC Comics']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(stats_out[1], int)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(stats_out[2], str)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> stats_out[3] in ['good', 'bad']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(stats_out[4], str)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= stats_out[5] <= 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(bhbe_out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bhbe_out.dtype == np.dtype('bool')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bhbe_out.sum()\n93",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0.5 <= obs_stat_out <= 1.0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(simulate_bhbe_out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> simulate_bhbe_out.shape[0]\n10",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ((0.45 <= simulate_bhbe_out) & (simulate_bhbe_out <= 1)).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(pval_out)\n2",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= pval_out[0] <= 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> pval_out[1] in ['Reject', 'Fail to reject']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
