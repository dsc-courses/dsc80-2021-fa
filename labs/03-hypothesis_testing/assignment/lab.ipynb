{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80 - Lab 03\n",
    "\n",
    "### Due Date: Monday October 18, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook may be graded (if it contains free response questions or asks you to draw plots).\n",
    "\n",
    "**Note**: Labs will have public tests and private tests. The public \"smoke tests\" that you will run below and which appear on Gradescope are generally worth no points. After the due date, we will replace these tests with private tests that will determine your grade. This is different from DSC 10, where labs only had public tests!\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name.\n",
    "- If you changed something you weren't supposed to, just use git to revert! Ask us if you need help with this, or google around for `git revert`.\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for *projects* will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file. You can write code here, but make sure that all of your real work is in the .py file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional helper functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetically speaking...\n",
    "\n",
    "In this section we'll develop an intuition for the terms and structure of hypothesis testing -- it's nothing to be afraid of!\n",
    "\n",
    "The first step is always to define what you're looking at, create your hypotheses, and set a level of significance.  Once you've done that, you can find a p-value which is related to your test statistic.\n",
    "\n",
    "If all of these words are scary: look at the lecture notebook, the textbook references, and don't forget to think about the real-world meaning of these terms!  The following example describes a real-world scenario, so you can think of it in a normal lens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Faulty tires**\n",
    "\n",
    "A tire manufacturer claims that their tires are so good, they will bring a Honda CRV from 60 mph to a complete stop in under 106 feet, 97% percent of the time.\n",
    "\n",
    "Now, you own a Honda CRV and this exact set of tires, and you decide to test this claim. You take your car to an empty Walmart parking lot, speed up to exactly 60 mph, hit the brakes, and measure your stopping distance. You repeat this 50 times (to the dismay of the other shoppers and the Walmart manager) and find that you stopped in under 106 feet only 47 of the times.\n",
    "\n",
    "Livid, you call the tire manufacturer and say that their claim was false. They say, no, that you were just unlucky: your experiment is consistent with their claim. But they didn't realize that they are dealing with a *data scientist*.\n",
    "\n",
    "To settle the matter, you decide to unleash the power of the hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will set up a hypothesis test in order to test your suspicion that the tires are are actually worse than claimed. Which of the following are valid null and alternative hypotheses for this hypothesis test?\n",
    "\n",
    "1. The tires will stop your car in under 106 feet exactly 97% of the time.\n",
    "0. The tires will stop your car in under 106 feet less than 97% of the time.\n",
    "0. The tires will stop your car in under 106 feet greater than 97% of the time.\n",
    "0. The tires will stop your car in more than 106 feet exactly 3% of the time.\n",
    "0. The tires will stop your car in more than 106 feet less than 3% of the time.\n",
    "0. The tires will stop your car in more than 106 feet greater than 3% of the time.\n",
    "\n",
    "Write a function `car_null_hypoth` which takes zero arguments and returns a list of the valid null hypotheses.\n",
    "Write a function `car_alt_hypoth` which takes zero arguments and returns a list of the valid alternative hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following are valid test statistics for our question?\n",
    "\n",
    "1. The number of times the car stopped in under 106 feet in 50 attempts.\n",
    "1. The average number of feet the car took to come to a complete stop in 50 attempts.\n",
    "1. The number of attempts it took before the car stopped in under 95 feet.\n",
    "1. The proportion of attempts the car successfully stopped in under 106 feet.\n",
    "\n",
    "Write a function `car_test_stat` which takes zero arguments and returns a list of valid test statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is calculated as how likely it is to find something as extreme or more extreme than our observed test statistic.  To do this, we assume the null hypothesis is true, and then define \"extremeness\" based on the alternative hypothesis.\n",
    "\n",
    "Why don't we just look at the probability of finding our observed test statistic?\n",
    "\n",
    "1. Because our observed test statistic isn't extreme.\n",
    "4. Because our null hypothesis isn't suggesting equality.\n",
    "5. Because our alternative hypothesis isn't suggesting equality.\n",
    "2. Because the probability of finding our observed test statistic equals the probability of finding something more extreme.\n",
    "3. Because if we run more and more trials (where a trial is speeding up the car then stopping), the probability of finding *any* observed test statistic gets closer and closer to zero, so if we did this we would always reject the null with more trials even if the null is true.\n",
    "\n",
    "\n",
    "Write a function `car_p_value` which takes zero arguments and returns the correct reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping: Google Play Store\n",
    "\n",
    "The questions below analyze a dataset of Google Play Store apps. The dataset has been preprocessed slightly for your convenience.\n",
    "\n",
    "Columns:\n",
    "* `App`: App Name\n",
    "* `Category`: App Category\n",
    "* `Rating`: Average App Rating\n",
    "* `Reviews`: Number of Reviews\n",
    "* `Size`: Size of App\n",
    "* `Installs`: Binned Number of Installs\n",
    "* `Type`: Paid or Free\n",
    "* `Price`: Price of App\n",
    "* `Content Rating`: Age group the app is targeted at\n",
    "* `Last Updated`: Last Updated Date\n",
    "\n",
    "\n",
    "Link: https://www.kaggle.com/lava18/google-play-store-apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "First, we'd like to do some basic cleaning to this dataset to better analyze it.\n",
    "In the function `clean_apps`, which takes the Play Store dataset as input, clean as follows and return the cleaned df:\n",
    "* Keep `Reviews` as type int.\n",
    "* Strip all letters from the ends of `Size`, convert all units to unit kilobyte, and convert the column to type float (Hint: all Sizes end in either M (megabyte) or k (kilobyte); a helper function may be useful here).\n",
    "* Strip the '+' from the ends of `Installs`, remove the commas, and convert it to type int.\n",
    "* Since `Type` is binary, change all the 'Free's to 1 and the 'Paid's to 0.\n",
    "* Strip dollar mark in `Price` and convert it to correct numeric data type.\n",
    "* Strip all but the year (e.g. 2018) from `Last Updated` and convert it to type int.\n",
    "\n",
    "Please return a *copy* of the original dataframe; don't alter the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (Continued)**\n",
    "\n",
    "Now, we can do some basic exploration.\n",
    "\n",
    "In the function `store_info`, find the following using the **cleaned** dataframe:\n",
    "* Find the year with the highest median `Installs`, among all years with at least 100 apps.\n",
    "* Find the `Content Rating` with the highest minimum `Rating`.\n",
    "* Find the `Category` has the highest average price.\n",
    "* Find the `Category` with lowest average rating, among apps that have at least 1000 reviews.\n",
    "\n",
    "and return these values in a list.\n",
    "\n",
    "*Remark:* Note that the last question is asking you to compute the *average of averages* (the 'Rating' column contains the average rating of an app) -- such analyses are prone to occurrences of Simpson's Paradox. Considering apps with at least 1000 reviews helps limit the effect of such [ecological fallacies](https://afraenkel.github.io/practical-data-science/05/understanding-aggregations.html#reversing-aggregations-ecological-fallacies).\n",
    "* You can assume there is no ties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = os.path.join('data', 'googleplaystore.csv')\n",
    "df = pd.read_csv(fp)\n",
    "cleaned = clean_apps(df)\n",
    "\n",
    "info = store_info(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Apps review count by App category\n",
    "\n",
    "A reasonable question that we may ask after cleaning the apps dataset is that how popular each app is. One way of measuring popularity of apps is by studying its review count within their respective category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "* Create a function `std_reviews_by_app_cat` that takes in a **cleaned** dataframe and outputs a dataframe with \n",
    "    - the same rows as the input,\n",
    "    - two columns given by `['Category', 'Reviews']`,\n",
    "    - where the `Reviews` columns are *standardized by app category* -- that is, the number of reviews for every app is put into the standard units for the category it belongs to. For a review of standard units, see the [DSC 10 Textbook](https://www.inferentialthinking.com/chapters/15/1/Correlation)\n",
    "    - *Hint*: use the methoc `groupby` and `transform`.\n",
    "* Lastly, create a function `su_and_spread` that returns a list of two items (hard-code your answers):\n",
    "    - Consider the following scenario: half of the apps in the category 'FAMILY' receives ratings of 0 stars while the other\n",
    "    half has rating of 5 stars. Similarly, the ‘MEDICAL' category has half 1-star and half 4-star apps.\n",
    "    Which app would have a higher rating after standarization? The five stars in the family category or the four stars in the\n",
    "    medical one. Answer with the name of the corresponding category ('FAMILY'/'MEDICAL') or use 'equal' if you think both\n",
    "    rating would be the same after standarization. (Don't worry about the uppercase but do be careful with the spelling). \n",
    "    - Which category type has the biggest \"spread\" of review count?\n",
    "    - Note: When calculating the standard deviation by hand, use the formula with `n` in the denominator. NumPy's `.std()` by default uses that formula, while `pd.Series().std()` by default uses the formula with `n - 1` in the denominator.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell -- it is needed for the tests\n",
    "fp = os.path.join('data', 'googleplaystore.csv')\n",
    "play = pd.read_csv(fp)\n",
    "cleaned = clean_apps(play)\n",
    "reviews_out = std_reviews_by_app_cat(cleaned)\n",
    "\n",
    "su_and_spread_out = su_and_spread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook Friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "A group of students decided to send out a survey to their Facebook friends. Each student asks 1000 of their friends for their first and last name, the company they currently work at, their job title, their email, and the university they attended. Combine all the data contained in the files `survey*.csv` (within the `responses` folder within the data folder) into a single dataframe. The number of files and the number of rows in each file may vary, so don't hardcode your answers!\n",
    "\n",
    "Create a function `read_survey` which takes in a directory path (containing files `survey*.csv`), and outputs a dataframe with six columns titled: `first name`, `last name`, `current company`, `job title`, `email`, `university` (in that order). \n",
    "\n",
    "*Hint*: You can list the files in a directory using `os.listdir`.\n",
    "\n",
    "*Remark: You may have to do some cleaning to make this possible!*\n",
    "\n",
    "Create a function `com_stats` that takes in in a dataframe and returns a (hardcoded) list containing: \n",
    "- The number of employees at the company that hired the most employees\n",
    "- The number of emails that end in \".edu\"\n",
    "- The job title that has the longest name (there are no ties)\n",
    "- The number of managers (hint: you may want to look through all the job titles to make sure you get all of them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell -- it is needed for the tests\n",
    "dirname = os.path.join('data', 'responses')\n",
    "out = read_survey(dirname)\n",
    "stats_out = com_stats(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Every week, a professor sends out an extra credit survey asking for students' favorite things (animals, movies, etc). \n",
    "- Each student who has completed at least 75% of the surveys receives 5 points of extra credit.\n",
    "- If at least 90% of the class answers at least one of the questions (ex. favorite animal), *everyone* in the class receives 1 point of extra credit. This overall class extra credit only applies once (ex. If 95% of students answer favorite color and 91% answer favorite animal, the entire class still only receives 1 extra point as a class).\n",
    "\n",
    "Create a function `combine_surveys` which takes in a directory path (containing files `favorite*.csv`) and combines all of the survey data into one DataFrame, indexed by student ID (a value 1 - 1000).\n",
    "\n",
    "Create a function `check_credit` which takes in a DataFrame with the combined survey data and outputs a DataFrame of the names of students and how many extra credit points they would receive, indexed by their ID (a value 1-1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell -- it is needed for the tests\n",
    "dirname = os.path.join('data', 'extra-credit-surveys')\n",
    "out = combine_surveys(dirname)\n",
    "check_credit_out = check_credit(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining pets and owners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "You are analyzing data from a veterinarian clinic. The datasets contain several types of information from the clinic, including its customers (pet owners), pets, and available procedures and history. The column names are self-explanatory. These dataframes are provided to you:\n",
    "-  `owners` stores the customer information, where every `OwnerID` is unique (verify yourself).\n",
    "-  `pets` stores the pet information. Each pet belongs to a customer in `owners`.\n",
    "-  `procedure_detail` contains a catalog of procedures that are offered by the clinic.\n",
    "-  `procedure_history` has procedure records. Each procedure is given to a pet in `pets`.\n",
    "\n",
    "You want to answer the following questions:\n",
    "\n",
    "1. What is the most popular Procedure Type for all of the pets we have in our `pets` dataset? Note that some pets are registered but haven't had any procedure performed. Also, some pets that have had procedures done, are not registered in `pets`. Create a function `most_popular_procedure` that takes in `pets`, `procedure_history` and returns the name of the most popular Procedure Type as a string.\n",
    " \n",
    "2. What is the name of each customer's pet(s)? Create a function `pet_name_by_owner` that takes in `owners`, `pets` and returns a Series that holds the pet name (as a string) indexed by owner's (first) name. If an owner has multiple pets, the corresponding value should be a list of names as strings.\n",
    "\n",
    "3. For each city that had owners who had their pets in our procedure history, how much does the city spend in total on procedures? Create a function `total_cost_per_city` that returns a Series that contains the sum of money that a city has spent on pets' procedures, indexed by `City`. Hint: think of what makes a procedure unique in the context of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell -- it is needed for the tests\n",
    "pets_fp = os.path.join('data', 'pets', 'Pets.csv')\n",
    "procedure_history_fp = os.path.join('data', 'pets', 'ProceduresHistory.csv')\n",
    "owners_fp = os.path.join('data', 'pets', 'Owners.csv')\n",
    "procedure_detail_fp = os.path.join('data', 'pets', 'ProceduresDetails.csv')\n",
    "pets = pd.read_csv(pets_fp)\n",
    "procedure_history = pd.read_csv(procedure_history_fp)\n",
    "owners = pd.read_csv(owners_fp)\n",
    "procedure_detail = pd.read_csv(procedure_detail_fp)\n",
    "\n",
    "out_01 = most_popular_procedure(pets, procedure_history)\n",
    "out_02 = pet_name_by_owner(owners, pets)\n",
    "out_03 = total_cost_per_city(owners, pets, procedure_history, procedure_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Line\n",
    "\n",
    "Before submitting your lab, make sure to run the doctests in the terminal with `python -m doctest lab.py`. If all of the tests in the notebook pass, but some fail when uploading to Gradescope, make sure that you've run the doctests in the terminal and they all pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(car_null_hypoth()) <= set(range(1,7))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(car_alt_hypoth()) <= set(range(1,7))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(car_test_stat()) <= set(range(1,5))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> car_p_value() in [1,2,3,4,5]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(cleaned) == len(df)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(info)\n4",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> info[2] in cleaned.Category.unique()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(reviews_out.columns) == set(['Category', 'Reviews'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(abs(reviews_out.select_dtypes(include='number').mean()) < 10**-7)  # standard units should average to 0!\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(su_and_spread_out) == 2\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> su_and_spread_out[0].lower() in ['medical', 'family', 'equal']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> su_and_spread_out[1] in ['ART_AND_DESIGN', 'AUTO_AND_VEHICLES', 'BEAUTY',\\\n...        'BOOKS_AND_REFERENCE', 'BUSINESS', 'COMICS', 'COMMUNICATION',\\\n...        'DATING', 'EDUCATION', 'ENTERTAINMENT', 'EVENTS', 'FINANCE',\\\n...        'FOOD_AND_DRINK', 'HEALTH_AND_FITNESS', 'HOUSE_AND_HOME',\\\n...        'LIBRARIES_AND_DEMO', 'LIFESTYLE', 'GAME', 'FAMILY', 'MEDICAL',\\\n...        'SOCIAL', 'SHOPPING', 'PHOTOGRAPHY', 'SPORTS', 'TRAVEL_AND_LOCAL',\\\n...        'TOOLS', 'PERSONALIZATION', 'PRODUCTIVITY', 'PARENTING', 'WEATHER',\\\n...        'VIDEO_PLAYERS', 'NEWS_AND_MAGAZINES', 'MAPS_AND_NAVIGATION']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(out, pd.DataFrame) == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(out) == 5000\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> try:\n...     read_survey('nonexistentfile')\n... except FileNotFoundError:\n...     print(True)\nTrue\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(stats_out) == 4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(stats_out[0], int)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(stats_out[2], str)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(out, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out.shape ==(1000, 6)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> try:\n...     combine_surveys('nonexistentfile')\n... except FileNotFoundError:\n...     print(True)\nTrue\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> check_credit_out.shape == (1000, 2)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(out_01, str)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(out_02) == len(owners)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'Sarah' in out_02.index\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'Cookie' in out_02.values\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(out_03.index) <= set(owners['City'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
