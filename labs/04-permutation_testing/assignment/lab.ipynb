{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Lab 04\n",
    "\n",
    "### Due Date: Monday October 25, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `lab*.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login time questions\n",
    "\n",
    "Imagine that you own an online store and you'd like to monitor the visits to your site. You've collected some data that you store in login_table.csv. It contains the information about different login dates and times for different users. Some users are unique, some visited your store multiple times.\n",
    "\n",
    "You need to answer a few questions below in order to understand the login patters of your users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Write a function `latest_login` which takes in a dataframe like `login` and outputs a dataframe, indexed by `Login Id`, of the login that occurs at the latest time-of-day for each user. Latest time-of-day is as it says: the latest time, **regardless of how recent the day is**.\n",
    "\n",
    "For example, if a user always logs in once per day at Noon, but her most recent log in happened to be at 8:00PM, then her latest log-in time becomes 8:00PM.\n",
    "\n",
    "Note: you do not need to import datetime. Look into how pandas has its own built in methods for dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = os.path.join('data', 'login_table.csv')\n",
    "login = pd.read_csv(fp)\n",
    "q1_result = latest_login(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "As a site owner, you would like to see how often users return to your site. You've noticed that there are users who have several logins and users who logged in only once. Of those users that logged in more than once, you are interested in finding the shortest amount of time elapsed between two consecutive logins for each of these users.\n",
    "\n",
    "Write a function `smallest_ellapsed` which takes in a dataframe like `login` and outputs a dataframe, indexed by Login ID, containing the shortest time elapsed for each user. Any users who haven't logged in more than once should not be included in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = os.path.join('data', 'login_table.csv')\n",
    "login = pd.read_csv(fp)\n",
    "q2_result = smallest_ellapsed(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "The pivot table allows you to group the entries of a dataframe into a two-dimensional table that provides a (multidimensional) summarization of the data. You are given a simple dataset, `sales.csv`, and are asked to solve a few simple problems using a `pivot table`.  \n",
    "\n",
    "We have provided the outline for your pivot tables. Your values will be different.\n",
    "\n",
    "* Write a function `total_seller` that takes `sales` dataframe and returns a pivot table that contains a total for each seller, indexed by a name.\n",
    "\n",
    "<img src=\"data/imgs/image_0.png\" width=\"15%\"/>\n",
    "\n",
    "* Write a function `product_name` that takes `sales` dataframe and returns a pivot table that contains a total for each seller, indexed by a product.\n",
    "\n",
    "<img src=\"data/imgs/image_1.png\" width=\"25%\"/>\n",
    "\n",
    "* Write a function `count_product` that takes `sales` dataframe and returns a pivot table that contains the total amount of items sold product wise, name wise per date. Replaces `NaNs` with 0s. \n",
    "\n",
    "<img src=\"data/imgs/image_2.png\" width=\"35%\"/>\n",
    "\n",
    "* Write a function `total_by_month` that takes `sales` dataframe and returns a pivot table that contains the total amount name wise, product wise per `month`. Replaces `NaNs` with 0s.\n",
    "\n",
    "<img src=\"data/imgs/image_3.png\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "Note: Here is a <a href = \"https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html\"> link </a> to a great source that provides an overview of the pivot tables with mane examples from the Titanic dataset. \n",
    "\n",
    "If your \"Total\" title is on the right side of the DataFrame instead of the left as seen above, as long as you are passing the doctests you should be fine. This is likely due to an issue with pandas versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A distribution of Skittles\n",
    "\n",
    "[Skittles](https://en.wikipedia.org/wiki/Skittles_(confectionery)) are made in two locations in the United States: Yorkville, Illinois and Waco, Texas. In these factories, Skittles of different colors are made separately by different machines and combined/packaged into bags for sale. The tab-separated file `skittles.tsv` contains the contents of 468 bags of Skittles.\n",
    "\n",
    "Most people have preferences for their favorite flavor and there is a surprising amount of variation among the distribution of flavors in each bag.\n",
    "\n",
    "Look at the variation by bag in the dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "skittles_fp = os.path.join('data', 'skittles.tsv')\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = os.path.join('data', 'sales.csv')\n",
    "df = pd.read_csv(fp)\n",
    "q3_total_seller_out = total_seller(df)\n",
    "q3_product_name_out = product_name(df)\n",
    "q3_product_count_out = count_product(df)\n",
    "q3_total_by_month_out = total_by_month(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between Yorkville and Waco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "First, you will investigate if the machine that mixes together the Skittles of different colors might favor one color over another. Use a permutation test to assess whether, on average, bags made in Yorkville have the same number of orange skittles as bags made in Waco. Do this by following the outline below:\n",
    "1. Create a function `diff_of_means` that takes in a dataframe of counts of skittles (like `skittles`) and their origin and returns the *absolute* difference of means between the number of orange Skittles per bag from Yorkville and Waco.\n",
    "2. Create a function `simulate_null` that takes in a dataframe of counts of skittles (like `skittles`) and their origin, and returns one instance of the test-statistic under the null hypothesis.\n",
    "3. Create a function `pval_orange` that takes in a dataframe of counts of skittles (like `skittles`) and their origin, and calculates the p-value for the permutation test using `1000` trials.\n",
    "\n",
    "Plot the observed statistic, along with the histogram for the simulated distribution, to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "skittles_fp = os.path.join('data', 'skittles.tsv')\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\\\t', engine='python')\n",
    "q4_diff_of_means_out = diff_of_means(skittles)\n",
    "q4_simulate_null_out = simulate_null(skittles)\n",
    "q4_pval_out = pval_orange(skittles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Use your work from above to decide which colors tend to differ the most between the two locations on average. Create a function `ordered_colors` that returns your answer as an ordered list from \"most different\" to \"least different\" between the two locations. Your list should be a *hard-coded* list, where each element has the form `(color, p-value)`.\n",
    "\n",
    "Even though there is randomness in the color composition in each bag, this list gives the likelihood that the machines have a systematic, meaningful, difference in how they blend the colors in each bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q5_out = ordered_colors()\n",
    "q5_colors = {'green', 'orange', 'purple', 'red', 'yellow'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Now, suppose you would like to assess whether the two locations make similar amounts of each color overall. That is:\n",
    "* Combine and count up all the Skittles of each color that were made in Yorkville \n",
    "* Combine and count up all the Skittles of each color that were made in Waco\n",
    "\n",
    "Are these distributions of colors similar? Is the variation among the bags due to each factory making different amounts of each color?\n",
    "\n",
    "Use a permutation test to assess whether the distribution of colors of Yorkville Skittles is statistically significantly different than those made in Waco. Set a significance level of 0.01 and determine whether you can reject a null hypothesis that answer the question above using a permutation test with 1000 trials. For your test statistic, use the total-variation-distance (TVD).\n",
    "\n",
    "Create a function `same_color_distribution` of zero variables that outputs a hard-coded tuple with the p-value and whether you 'Fail to Reject' or 'Reject' the null hypothesis.\n",
    "\n",
    "For this question, the following references may be useful:\n",
    "* For TVD reference, see [DSC 10](https://www.inferentialthinking.com/chapters/11/2/Multiple_Categories.html) and [Lecture 04](https://github.com/dsc-courses/dsc80-2021-fa/tree/main/lectures/04-hypothesis_testing/notebook)\n",
    "* For permutation test reference, see [DSC 10](https://www.inferentialthinking.com/chapters/12/Comparing_Two_Samples.html) and [Lecture 07](https://github.com/dsc-courses/dsc80-2021-fa/tree/main/lectures/07-permutation_tests/notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q6_out = same_color_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis vs Permutation Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "In each of the following scenarios, decide  whether  a  permutation test is appropriate to determine if there is a  significant difference between the quantities described. If a permutation test is appropriate, mark 'P'. Otherwise, mark 'H'.\n",
    "\n",
    "Record your answers in the function `perm_vs_hyp` that outputs a list of length 5, containing the values 'H' and 'P\"\n",
    "\n",
    "1. Compare the DSC 80 pass rate between second years and third years who take the class.\n",
    "    - Permutation or Hypothesis test?\n",
    "2. Compare the proportion of Data Science majors who have completed DSC 80 and the proportion of Data Science minors who have completed DSC 80.\n",
    "    - Permutation or Hypothesis test?\n",
    "3. Compare the proportion of students who have iPhones to the proportion of students who have Android phones.\n",
    "    - Permutation or Hypothesis test?\n",
    "4. Out of the DSC 80 students, the professor asks students whether they prefer DSC 10 or DSC 20.  Compare the proportion of students who prefer DSC 10 to the proportion who prefer DSC 20.\n",
    "    - Permutation or Hypothesis test?\n",
    "5. Compare the attendance rate of classes that use iClickers vs classes that do not use iClickers.\n",
    "    - Permutation or Hypothesis test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q7_out = perm_vs_hyp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Missingness\n",
    "\n",
    "### Missing by Design (MD)\n",
    "- The missing field is deliberately missing. The missing field is deliberately not collected or set to null (hence, \"missing by design\")\n",
    "- The missingness can be exactly predicted when a column will be null, with only knowledge of the other columns using a function of the rows of the dataset\n",
    "\n",
    "### Missing Completely at Random (MCAR)\n",
    "- The missingness of missing value isn't related to the actual, unreported value itself, nor the values in any other fields. The missingness is not systematic.\n",
    "- The missingness is unconditionally uniform across rows. MCAR doesn't bias the observed data.\n",
    "- There is no relationship between the missing data and the any of the other data, observed or missing.\n",
    "\n",
    "### Missing at Random (MAR)\n",
    "- The missingness of the missing value has nothing to do with the value itself, but may be related to another field.\n",
    "- The missingness is uniform across rows, perhaps conditional on another column. MAR biases the observed data, but is fixable.\n",
    "- There is a systematic relationship between the missing values and the observed data (but not the missing values themselves).\n",
    "- Difference between MD and MAR: If you can *exactly/always* determine missingness on other columns, the missingness is MD. If there is just some sort of systematic relationship between the missing columns/values and other columns/values that may help us predict missingness, the missingness is MAR.\n",
    "\n",
    "### Non-Ignorable (NI)\n",
    "- The missingness of the missing value is related to the actual, unreported value.\n",
    "- NI biases the observed data in unobservable ways.\n",
    "- There is relationship between the propensity of a value to be missing and its value.\n",
    "\n",
    "*Note:* In class, we sometimes refer to non-ignorable missingness as \"not missing at random (NMAR)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "### After-purchase surveys\n",
    "\n",
    "You run a small e-commerce website and send surveys out to customers after they purchase an item from your store. The survey asks whether the customer is satisfied with their purchase (\"Yes\" or \"No\"). Below, you are presented with possible datasets, each of which contains a column `satisfied` as described above, as well as a `customer_id` number corresponding to the order. The column `satisfied` is missing data. \n",
    "\n",
    "For each of the following datasets, label the column `satisfied` as being `MD`, `MCAR`, `MAR`, `NI`.\n",
    "\n",
    "1. The dataset consists only of the columns `customer_id` and `satisfied`.\n",
    "2. The dataset contains the `customer_id` of every customer with an account, even if they didn't make a purchase. Also, in this case, you notice everyone who was sent a survey filled it out.\n",
    "3. The dataset contains a column specifying if the user later returned the item.\n",
    "4. The dataset contains a column with the serial number for the item purchased.\n",
    "5. The dataset contains a column with the price of the item purchased.\n",
    "\n",
    "Record your answers in the function `after_purchase` that outputs a list of length 5, containing the values `MD`, `MCAR`, `MAR`, `NI`. For some questions there may be multiple good answer, but there is generally one answer that is \"best\". If you are unsure, ask a tutor, but be prepared to provide justification for whichever answer(s) you think might be right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q8_out = after_purchase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**\n",
    "\n",
    "### Miscellaneous missingness questions\n",
    "\n",
    "In each of the following scenarios, choose the best answer. Return your answers in a function `multiple_choice`.\n",
    "\n",
    "1. UCSD has recently adopted GrubHub as the food pre-ordering app for campus restaurants, so you can order your food ahead of time and stop by before your next class. In a table of GrubHub app orders, which contains information such as `restaurant`, `name`, `items`, and `total`, the column `delivery_address` is often missing for UCSD students. The missingness mechanism of these columns is likely:\n",
    "    - Is the exam grade column `MD`, `MCAR`, `MAR`, `NI`?\n",
    "1. In a database of student records that records student profile data, such as `name`, `home_address`, `ethnicity`, etc., sometimes the Middle Name column is missing. This column is most likely:\n",
    "    - `MD`, `MCAR`, `MAR`, `NI`\n",
    "1. The UCSD Club Basketball team creates a signup sheet for potential new members. The sheet contains the columns: `full_name`, `year`, `email`, `favorite_sports`, `number_of_sports_played`, `sports_previously_played`. The team president notices that many students left the `sports_previously_played` blank. The missingness mechanism of this column is likely:\n",
    "    - `MD`, `MCAR`, `MAR`, `NI`\n",
    "1. Associated Students sends out a survey to all students about their 2019 Sun God Experience, with all questions being optional. They notice that many students left the \"Were you satisfied with Sun God 2019?\". This missingness is most likely:\n",
    "    - `MD`, `MCAR`, `MAR`, `NI`\n",
    "1. UC San Diego is implementing two-step login through DUO on October 16th. On October 1st, an administrator creates a table of randomized student codes for all students (not their PID) and the phone numbers associated with their DUO account (two columns total). The administrator notices that there is a lot of missingness in the phone numbers column. This column is most likely:\n",
    "    - `MD`, `MCAR`, `MAR`, `NI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q9_out = multiple_choice()\n",
    "q9_ans = ['MD', 'MCAR', 'MAR', 'NI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Line\n",
    "\n",
    "Before submitting your lab, make sure to run the doctests in the terminal with `python -m doctest lab.py`. If all of the tests in the notebook pass, but some fail when uploading to Gradescope, make sure that you've run the doctests in the terminal and they all pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q1_result)\n433",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q1_result.loc[381, \"Time\"].hour > 12\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q2_result)\n238",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 18 < q2_result.loc[1233, \"Time\"].days < 23\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3_total_seller_out.index.dtype\ndtype('O')",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3_total_seller_out[\"Total\"].sum() < 15000\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3_product_name_out.size\n15",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3_product_name_out.loc[\"pen\"].isnull().sum()\n0",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3_product_count_out.loc[\"boat\"].loc[\"Trump\"].value_counts()[0]\n6",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3_product_count_out.size\n70",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3_total_by_month_out[\"Total\"][\"May\"].idxmax()\n('Smith', 'book')",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3_total_by_month_out.shape[1]\n5",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 <= q4_diff_of_means_out\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(q4_simulate_null_out, float)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= q4_simulate_null_out <= 1.0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(q4_pval_out, float)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= q4_pval_out <= 0.1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q5_out)\n5",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set([x[0] for x in q5_out]) == q5_colors\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all([isinstance(x[1], float) for x in q5_out])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q6_out[0], float)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q6_out[1] in ['Fail to Reject', 'Reject']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q7_out) == 5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(q7_out) <= set(['P', 'H'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q8_out) == 5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(q8_out) <= set(['MD', 'MCAR', 'MAR', 'NI'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q9_out) == 5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(q9_out) <= set(q9_ans)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q9_out[1] in q9_ans\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
